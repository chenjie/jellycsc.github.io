<!DOCTYPE html>
<html>

  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <title>深入理解LSTM记忆元网络</title>
    <meta name="description" content="本文翻译源自Google Brain机器学习研究员colah的博客，原文链接请点击这里。">

    <script src=" /js/jquery.min.js " charset="utf-8"></script>

    <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
    <link rel="stylesheet" href="https://cdn.bootcss.com/font-awesome/4.7.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="https://at.alicdn.com/t/font_8v3czwksspqlg14i.css">
    <link rel="stylesheet" href="/css/main.css ">
    <link rel="canonical" href="http://localhost:4000/2018/04/13/understanding-lstm-networks/">
    <link rel="alternate" type="application/rss+xml" title="JellyCSC" href="http://localhost:4000/feed.xml ">


    <script>
    // 百度统计代码
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "//hm.baidu.com/hm.js?0d7c5d2c7e619c2520a48a2d7b45bf34";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
    </script>


    <script>
    // google analytics
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

      ga('create', 'UA-117352524-1', 'auto');
      ga('require', 'displayfeatures');
      ga('send', 'pageview');

    </script>


<script type="text/javascript">
function detect_ResizeMathJax() {
    var min_ratio = 1;
    jQuery('.MathJax_Display').each(function(ii, obj) {
      var latex = obj.children[0];
      var w = latex.offsetWidth;
      var h = latex.offsetHeight;    
      var W = obj.offsetWidth;
      if (w > W) {
        if (W / w < min_ratio){
          min_ratio = W / w;
        }
      }
    })
  return min_ratio;};

// resize all LaTeX Display elements to they fit in on screen
function cvonk_ResizeMathJax(ratio) {
    jQuery('.MathJax_Display').each(function(ii, obj) {
        obj.style.fontSize = 100 * ratio + "%";
    });
}
window.MathJax = {
    AuthorInit: function() {
  MathJax.Hub.Register.StartupHook("Begin", function() {
      MathJax.Hub.Queue(function() {
        cvonk_ResizeMathJax(detect_ResizeMathJax());
      });
  });
    },
    jax: ["input/TeX", "output/HTML-CSS", "output/NativeMML"],
    extensions: ["tex2jax.js"]
};
// window.addEventListener("resize", function() {
//     cvonk_ResizeMathJax(detect_ResizeMathJax()); 
// });
window.addEventListener("orientationchange", function() {
    cvonk_ResizeMathJax(detect_ResizeMathJax()); 
});

</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
    tex2jax: { inlineMath: [["$","$"],["\\(","\\)"]] },
    "HTML-CSS": {
      linebreaks: { width: "container" }
    }
});
</script>

<script type="text/javascript"
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

</head>


  <body>

    <header id="top">
    <div class="wrapper">
        <a href="/" class="brand">JellyCSC</a>
        <small>Software Developer</small>
        <button id="headerMenu" class="menu"><i class="fa fa-bars"></i></button>
        <nav id="headerNav">
            <ul>
                <li>
                    
                    <a href="/">
                    
                        <i class="fa fa-home"></i>Home
                    </a>
                </li>

                
                    
                    <li>
                        
                        <a href="/category/">
                        
                            <i class="fa fa-th-list"></i>Categories
                        </a>
                    </li>
                    
                
                    
                    <li>
                        
                        <a href="/collection/">
                        
                            <i class="fa fa-bookmark"></i>Collections
                        </a>
                    </li>
                    
                
                    
                    <li>
                        
                        <a href="/about/">
                        
                            <i class="fa fa-heart"></i>About Me
                        </a>
                    </li>
                    
                
                    
                    <li>
                        
                        <a href="/tag/">
                        
                            <i class="fa fa-tags"></i>Tags
                        </a>
                    </li>
                    
                
                    
                    <li>
                        
                        <a href="/archive/">
                        
                            <i class="fa fa-archive"></i>Archives
                        </a>
                    </li>
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
            </ul>
        </nav>
    </div>
</header>


        <div class="page clearfix" post>
    <div class="left">
        <h1>深入理解LSTM记忆元网络</h1>
        <div class="label">

            <div class="label-card">
                <i class="fa fa-calendar"></i>2018-04-13
            </div>

            <div class="label-card">
                
            </div>

            <div class="label-card">
                
            </div>

            <div class="label-card">
            


<!-- <span class="point">•</span> -->
<span class="categories">
  <i class="fa fa-th-list"></i>
  
    
        <a href="/category/#ML" title="Category: ML" rel="category">ML</a>
    
  

  <!-- <span class="point">•</span> -->
</span>


            </div>

            <div class="label-card">
            
<!-- <span class="point">•</span> -->
<span class="pageTag">
  <i class="fa fa-tags"></i>
  
    
        <!--a href="/tag/#RNN" title="Tag: RNN" rel="tag">RNN</a-->
        <a href="/tag/#RNN" title="Tag: RNN" rel="tag">RNN</a>
    
  

</span>

            </div>

        </div>
        <hr>
        <article itemscope itemtype="http://schema.org/BlogPosting">
        <ul id="markdown-toc">
  <li><a href="#rnn循环神经网络" id="markdown-toc-rnn循环神经网络">RNN循环神经网络</a></li>
  <li><a href="#长期long-term依赖的问题" id="markdown-toc-长期long-term依赖的问题">长期(Long-Term)依赖的问题</a></li>
  <li><a href="#lstm记忆元网络" id="markdown-toc-lstm记忆元网络">LSTM记忆元网络</a></li>
  <li><a href="#lstm记忆元网络核心思想" id="markdown-toc-lstm记忆元网络核心思想">LSTM记忆元网络核心思想</a></li>
  <li><a href="#一步一步了解lstm记忆元" id="markdown-toc-一步一步了解lstm记忆元">一步一步了解LSTM记忆元</a></li>
  <li><a href="#lstm记忆元的不同版本" id="markdown-toc-lstm记忆元的不同版本">LSTM记忆元的不同版本</a></li>
  <li><a href="#总结" id="markdown-toc-总结">总结</a></li>
  <li><a href="#感谢" id="markdown-toc-感谢">感谢</a></li>
  <li><a href="#注释" id="markdown-toc-注释">注释</a></li>
</ul>

<p>本文翻译源自<a href="https://ai.google/brain-team/">Google Brain</a>机器学习研究员<a href="https://github.com/colah/">colah</a>的<a href="http://colah.github.io/">博客</a>，原文链接请点击<a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/">这里</a>。</p>

<h2 id="rnn循环神经网络">RNN循环神经网络</h2>
<p>人们每一秒思考都是连续的，而并非从零开始。当你在读这篇文章的时候，你通过先前的词句来理解当前词语的含义。你读到一半不会扔掉前面所有获取到的文章信息而从头开始思考，这说明了你的思维是连贯的。</p>

<p>传统的神经网络正因为不能做到这个，所以这也似乎是它的缺陷。举个例子，假如你想要用神经网络来将每一个电影的时间点所发生的事件分类，你不清楚传统的神经网络是如何将它对于之前事件的推理传递到之后的事件中去。</p>

<p>循环神经网络(<a href="https://en.wikipedia.org/wiki/Recurrent_neural_network">RNN</a>)解决了这个问题，他们在自身的网络中拥有循环结构，使得不同时间点(timestamp)的推理信息得以保存传递下来。</p>

<p align="center">
<img src="/mdres/posts/2018/lstm/RNN-rolled.png" width="20%" /> <br />
<strong>带有循环结构的RNN模型</strong> </p>

<p>在上图中，$A$代表任何神经网络(CNN, MLP…)，$X_t$代表在$t$时间点的输入，$h_t$代表在$t$时间点的输出。图中的循环结构让信息能够从RNN的一个时间点传递到之后的时间点去。</p>

<p>这些循环结构让RNN看上去有些神秘，但是如果你再仔细思考一下，就会发现其实RNN和普通的神经网络(CNN, MLP…)没有太大差别。RNN可以被看作是一个神经网络的多个镜像，每一个镜像为它连接的下一个镜像传递信息。设想一下RNN的循环结构展开是什么样子的？</p>

<p align="center"> <br />
<img src="/mdres/posts/2018/lstm/RNN-unrolled.png" width="90%" /> <br />
<strong>循环结构被展开的RNN模型</strong> </p>

<p>RNN链式结构的本质展现了它和序列(sequences &amp; lists)是紧密相连的，所以它自然就是序列这种数据类型最合适的神经网络架构模型。</p>

<p>RNN其实就在我们身边，一直在被人们所使用着。近年来(2015)，RNN的应用在以下问题领域中取得了突出成就：语音识别(speech recognition)，语言建模(language modeling)，翻译(translation)，看图说话/图像标注(image captioning)等等，这个列表还在一直延续下去。在<a href="https://cs.stanford.edu/people/karpathy/">Andrej Karpathy</a>的<a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/">这篇出色的博客</a>中论述了RNN非凡的功绩。</p>

<p>RNN成功的关键就是LSTM(Long short-term memory)记忆元“们”，不计其数的LSTM记忆元组成了一个十分特别的循环神经网络，我(译者)将它称作为LSTM记忆元网络，它在很多任务中的表现比标准RNN更加强大，几乎所有令人瞩目的RNN成就的功臣都是它。接下来我们就对LSTM记忆元展开进一步探索。</p>

<h2 id="长期long-term依赖的问题">长期(Long-Term)依赖的问题</h2>
<p>RNN的吸引力之一在于这样一种将之前的信息用于当前判断/抉择的想法，就像视频是连贯的，先前的视频帧对于理解当前的视频画面有很大的帮助和提示。如果RNN也能做到这样，它们会变得非常有用。但是它们真的可以做到吗？还不一定。</p>

<p>有时候我们只需要近(短)期的信息来完成当前的任务，就像在语言模型中预测下一个词语基于先前的一些词语。如果我们要尝试预测<code class="highlighter-rouge">The clouds are in the __.</code>这句话的最后一个词，我们不需要任何更多的上下文信息，答案很明显就应该是<code class="highlighter-rouge">sky</code>(因为云绝大多数情况下在天上)。在这种，答案就藏在前后几个词的情况下，标准RNN也能够学习训练用先前信息作出正确判断。</p>

<p align="center">
<img src="/mdres/posts/2018/lstm/RNN-shorttermdepdencies.png" width="70%" /> <br />
<strong>短期(Short-Term)依赖的RNN模型</strong> </p>

<p>但是绝大多数情况下答案不会藏在前后几个词中，我们需要更多的上下文信息来帮助作出准确的预测。这次如果我们要尝试预测<code class="highlighter-rouge">I grew up in France ..(此处省略一段话).. I speak fluent __.</code>这段话的最后一个词，首先我们可以通过前后词的信息来确定这应该是一门语言的名字，但是如果要具体到究竟是哪一门语言，我们就需要上文中(一段话之前的)<code class="highlighter-rouge">France</code>的信息。在我们日常的语言中，完全有可能出现像这样的相关信息<code class="highlighter-rouge">I grew up in France</code>，离预测位置<code class="highlighter-rouge">I speak fluent __.</code>很远的情况。</p>

<p>很遗憾的是，当这个距离拉开的越来越大，标准RNN很难再通过训练学习来连接<code class="highlighter-rouge">相关信息</code>和<code class="highlighter-rouge">预测位置</code>的关系，从而就鲜有令人满意的预测了。</p>

<p align="center">
<img src="/mdres/posts/2018/lstm/RNN-longtermdependencies.png" width="90%" /> <br />
<strong>长期(Long-Term)依赖的RNN模型</strong> </p>

<p>从理论上来说，标准RNN完全有能力处理长期依赖关系，但是现实中为了解决简单的问题，也需要人工精确地调整其参数，而RNN自己似乎无法将那些参数学习到。这个问题在以下两篇论文中都有深入的探讨：<a href="/mdres/posts/2018/lstm/SeppHochreiter1991ThesisAdvisorSchmidhuber.pdf">Hochreiter (1991) [German]</a>，<a href="/mdres/posts/2018/lstm/tnn-94-gradient.pdf">Bengio, et al. (1994)</a> 他们找到了一些关于这个问题如此困难的根本原因。</p>

<p>幸好，LSTM没有这个问题。</p>

<h2 id="lstm记忆元网络">LSTM记忆元网络</h2>
<p>LSTM记忆元网络(通常简称为<code class="highlighter-rouge">LSTMs</code>)是一种特别的RNN模型，它擅长捕捉学习长期的依赖关系。它是由<a href="/mdres/posts/2018/lstm/2604.pdf">Hochreiter &amp; Schmidhuber (1997)</a>提出的，并且在接下来的工作中被许多人提炼和推广<sup>1</sup>。LSTM记忆元结构在各种各样的问题上都获得了显著的成效，现在被广泛使用。</p>

<p>LSTM设计之初是用来避免长期依赖问题的，长期地记忆信息是LSTM记忆元的缺省模式，并不是它们要刻意去学习的。</p>

<p>所有RNN都具有神经网络的重复链式模组结构(重复代表它展开后每一个timestamp的结构都一模一样，链式代表它们之间相互连接传递信息，模组代表前面说的每一个timestamp的结构)，在标准的RNN模型中，这个重复的模组式结构具有非常简单的形态，比如只有一个$tanh$激活函数层：</p>

<p align="center"> <br />
<img src="/mdres/posts/2018/lstm/LSTM3-SimpleRNN.png" width="90%" /> <br />
<strong>标准RNN重复模组式结构中的$tanh$激活层</strong> </p>

<p>LSTM记忆元网络也有与标准RNN相似的链式结构，但是重复的模组式结构有不同的形态。它不是单单一层$tanh$，而是拥有四(4)个元素以特殊逻辑形式交互的复杂层。[译者注：这里参考了<a href="https://blog.csdn.net/yujianmin1990">lorderYu</a>在其<a href="https://blog.csdn.net/yujianmin1990/article/details/78826506">CSDN博客</a>中的翻译，用词非常精准，感谢！]</p>

<p align="center"> <br />
<img src="/mdres/posts/2018/lstm/LSTM3-chain.png" width="90%" /> <br />
<strong>一个LSTM记忆元是包括四(4)个元素以特殊逻辑形式交互的复杂层</strong> </p>

<p>不用担心图中的细节都表示什么，我们会一步一步讲解上图LSTM的含义。现在让我们先认识下需要使用的一些符号：</p>

<p align="center">
<img src="/mdres/posts/2018/lstm/LSTM2-notation.png" width="80%" /> <br /> </p>

<p>在上图中，每条线都代表一个多维向量，从一个节点的输出端流向其他节点的输入端。粉色的圆圈代表逐点计算(element-wise or point-wise operation)，比如向量的加法。</p>

<script type="math/tex; mode=display">[译者注：(x_1, x_2) + (y_1, y_2) = (x_1 + y_1, x_2 + y_2)]</script>

<p>黄色的矩形代表神经网络层。箭头汇合代表向量合并为多维矩阵，箭头分支表示数据流复制并流向不同路径。</p>

<h2 id="lstm记忆元网络核心思想">LSTM记忆元网络核心思想</h2>
<p>LSTMs的关键是cell状态，那条在结构图上侧的水平线。cell状态有点像是传送带，在整个链中从头流到尾，只有很少线性操作插入进来。对信息而言，可以很容易地通过它无损地流通。</p>

<p align="center">
<img src="/mdres/posts/2018/lstm/LSTM3-C-line.png" width="100%" /> <br /> </p>

<p>LSTM通过称之为门的结构来控制将信息从cell状态中加入或者移除。门是种可选择地控制信息流通的方式，它们由sigmoid神经层和pointwise乘积操作组成。</p>

<p align="center">
<img src="/mdres/posts/2018/lstm/LSTM3-gate.png" width="20%" /> <br /> </p>

<p>sigmoid神经层输出介于0和1之间的值，描述了每个元素应该通过的比例。0意味着拒绝任何信息通过，1意味着让信息无损通过。一个LSMT有三个这种门，来保护和控制cell状态。</p>

<h2 id="一步一步了解lstm记忆元">一步一步了解LSTM记忆元</h2>
<p>LSTM的第一步是决定什么信息将要从cell状态中丢掉。这一决定由sigmoid组成的“forget gate”来搞定，它输入ht−1和xt，输出与cell状态Ct−1同维度的0/1向量。</p>

<p>再回头想下前面的语言模型预测的例子，cell状态可能包含当前主题的性别，所以当前代称可以正确地判断出来。当看到一个新的主题时，我们想遗忘掉旧主题的性别信息。　　</p>

<p align="center">
<img src="/mdres/posts/2018/lstm/LSTM3-focus-f.png" width="90%" /> <br /> </p>

<p>下一步是决定哪些新信息要被存储到cell状态中。分两部分，第一，由sigmoid组成“input gate”决定哪些维度将要被更新；第二，由tanh生成心得可以被加入到cell状态中的候选值向量Ct~。然后，将这两个向量合并生成对cell状态更新的向量。</p>

<p>在语言模型预测的那个例子中，我们将增加新主题的性别信息到cell状态中，以替换掉旧的性别信息（被忘掉的那个）。</p>

<p align="center">
<img src="/mdres/posts/2018/lstm/LSTM3-focus-i.png" width="90%" /> <br /> </p>

<p>现在，将旧的cell状态Ct−1更细为Ct，上面几步已经给出了要做的事情。</p>

<p>首先，将旧cell状态乘以ft，忘掉我们决定要遗忘掉的内容；然后，叠加it∗Ct~，那是我们决定要追加的更新内容。
在语言预测模型的例子中，上述动作对应实际要丢弃的关于旧主题的性别信息，并增加新的信息。　</p>

<p align="center">
<img src="/mdres/posts/2018/lstm/LSTM3-focus-C.png" width="90%" /> <br /> </p>

<p>最后，我们需要决定输出什么，这个输出基于cell状态的，但是个过滤后的版本。先用sigmoid（“output gate”）决定cell状态的哪些部分是将要输出的，以及对应的输出比例；再用tanh将cell状态投射到[-1,1]之间；再将两者乘积得到我们想要输出的结果。</p>

<p>对语言预测例子而言，当LSTM刚看到一个主题，它想输出动词相关的信息时，比如，它可能输出单数/复数形式的主题，所以需要知道什么类型的动词的格式应该被对应地加入后续要发生的动作中。　　</p>

<p align="center">
<img src="/mdres/posts/2018/lstm/LSTM3-focus-o.png" width="90%" /> <br /> </p>

<h2 id="lstm记忆元的不同版本">LSTM记忆元的不同版本</h2>
<p>What I’ve described so far is a pretty normal LSTM. But not all LSTMs are the same as the above. In fact, it seems like almost every paper involving LSTMs uses a slightly different version. The differences are minor, but it’s worth mentioning some of them.</p>

<p>One popular LSTM variant, introduced by Gers &amp; Schmidhuber (2000), is adding “peephole connections.” This means that we let the gate layers look at the cell state.</p>

<p align="center">
<img src="/mdres/posts/2018/lstm/LSTM3-var-peepholes.png" width="90%" /> <br /> </p>

<p>The above diagram adds peepholes to all the gates, but many papers will give some peepholes and not others.</p>

<p>Another variation is to use coupled forget and input gates. Instead of separately deciding what to forget and what we should add new information to, we make those decisions together. We only forget when we’re going to input something in its place. We only input new values to the state when we forget something older.</p>

<p align="center">
<img src="/mdres/posts/2018/lstm/LSTM3-var-tied.png" width="90%" /> <br /> </p>

<p>A slightly more dramatic variation on the LSTM is the Gated Recurrent Unit, or GRU, introduced by Cho, et al. (2014). It combines the forget and input gates into a single “update gate.” It also merges the cell state and hidden state, and makes some other changes. The resulting model is simpler than standard LSTM models, and has been growing increasingly popular.</p>

<p align="center">
<img src="/mdres/posts/2018/lstm/LSTM3-var-GRU.png" width="90%" /> <br /> </p>

<p>These are only a few of the most notable LSTM variants. There are lots of others, like Depth Gated RNNs by Yao, et al. (2015). There’s also some completely different approach to tackling long-term dependencies, like Clockwork RNNs by Koutnik, et al. (2014).</p>

<p>Which of these variants is best? Do the differences matter? Greff, et al. (2015) do a nice comparison of popular variants, finding that they’re all about the same. Jozefowicz, et al. (2015) tested more than ten thousand RNN architectures, finding some that worked better than LSTMs on certain tasks.</p>

<h2 id="总结">总结</h2>
<p>Earlier, I mentioned the remarkable results people are achieving with RNNs. Essentially all of these are achieved using LSTMs. They really work a lot better for most tasks!</p>

<p>Written down as a set of equations, LSTMs look pretty intimidating. Hopefully, walking through them step by step in this essay has made them a bit more approachable.</p>

<p>LSTMs were a big step in what we can accomplish with RNNs. It’s natural to wonder: is there another big step? A common opinion among researchers is: “Yes! There is a next step and it’s attention!” The idea is to let every step of an RNN pick information to look at from some larger collection of information. For example, if you are using an RNN to create a caption describing an image, it might pick a part of the image to look at for every word it outputs. In fact, Xu, et al. (2015) do exactly this – it might be a fun starting point if you want to explore attention! There’s been a number of really exciting results using attention, and it seems like a lot more are around the corner…</p>

<p>Attention isn’t the only exciting thread in RNN research. For example, Grid LSTMs by Kalchbrenner, et al. (2015) seem extremely promising. Work using RNNs in generative models – such as Gregor, et al. (2015), Chung, et al. (2015), or Bayer &amp; Osendorfer (2015) – also seems very interesting. The last few years have been an exciting time for recurrent neural networks, and the coming ones promise to only be more so!</p>

<h2 id="感谢">感谢</h2>
<p>I’m grateful to a number of people for helping me better understand LSTMs, commenting on the visualizations, and providing feedback on this post.</p>

<p>I’m very grateful to my colleagues at Google for their helpful feedback, especially Oriol Vinyals, Greg Corrado, Jon Shlens, Luke Vilnis, and Ilya Sutskever. I’m also thankful to many other friends and colleagues for taking the time to help me, including Dario Amodei, and Jacob Steinhardt. I’m especially thankful to Kyunghyun Cho for extremely thoughtful correspondence about my diagrams.</p>

<p>Before this post, I practiced explaining LSTMs during two seminar series I taught on neural networks. Thanks to everyone who participated in those for their patience with me, and for their feedback.</p>

<h2 id="注释">注释</h2>
<ol>
  <li>除了原作者以外，还有很多人为现代LSTM做出了贡献。以下是一份不完全名单：Felix Gers, Fred Cummins, Santiago Fernandez, Justin Bayer, Daan Wierstra, Julian Togelius, Faustino Gomez, Matteo Gagliolo, and <a href="https://scholar.google.com/citations?user=DaFHynwAAAAJ&amp;hl=en">Alex Graves</a>.</li>
</ol>

<p><img src="/mdres/loading.gif" width="20" />持续更新中。。。</p>

        </article>
        <hr>

        
        
            
            
                
                    
                
            
        
            
            
                
                    
                
            
                
                    
                
            
        
            
            
                
                    
                
            
        
            
            
                
                    
                
            
                
                    
                
            
        
            
            
                
                    
                
            
        
            
            
                
                    
                
            
                
                    
                
            
        
            
            
                
                    
                
            
                
                    
                
            
                
                    
                
            
        
        

        <div class="post-recent">
    <div class="pre">
        
        <p><span>&#9999;</span><strong>之前那篇: </strong> <a href="/2018/04/13/xor-and-or-not/">XOR using NAND, AND, OR</a></p>
        
    </div>
    <div class="nex">
        
    </div>
</div>


        <h2 id="comments">Comments</h2>
        


<div id="disqus_thread"></div>
<script>
    /**
     * RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
     * LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables
     */

    var disqus_config = function() {
        this.page.url = 'http://localhost:4000/2018/04/13/understanding-lstm-networks/'; // Replace PAGE_URL with your page's canonical URL variable
        this.page.identifier = 'http://localhost:4000/2018/04/13/understanding-lstm-networks/'; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
    };

    (function() { // DON'T EDIT BELOW THIS LINE
        var d = document,
            s = d.createElement('script');

        s.src = '//jellycsc.disqus.com/embed.js';

        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript>




    </div>
    <button class="anchor"><i class="fa fa-anchor"></i></button>
    <div class="right">
        <div class="wrap">

            <!-- Content -->
            <div class="side content">
                <div>
                    Content
                </div>
                <ul id="content-side" class="content-ul">
                    
                    <li><a href="#comments">Comments</a></li>
                </ul>
            </div>
            <!-- 其他div框放到这里 -->
            <!-- <div class="side">bbbb</div> -->
        </div>
    </div>
</div>
<script>
/**
 * target _blank
 */
(function() {
    var aTags = document.querySelectorAll('article a:not([id])')
    for (var i = 0; i < aTags.length; i++) {
        aTags[i].setAttribute('target', '_blank')
    }
}());
</script>
<script src="/js/pageContent.js " charset="utf-8"></script>


    <footer class="site-footer">


    <div class="wrapper">

        <p class="description">
            
        </p>
        <p class="contact">
            
            <a href="https://github.com/jellycsc" title="GitHub"><i class="fa fa-github" aria-hidden="true"></i></a>  
            <a href="mailto:nichenjie2013@gmail.com" title="email"><i class="fa fa-envelope-o" aria-hidden="true"></i></a>     
            <a href="https://www.facebook.com/chenjie.ni.96" title="Facebook"><i class="fa fa-facebook-official" aria-hidden="true"></i></a>   
            <a href="https://www.linkedin.com/in/chenjie-ni-b1613614a" title="LinkedIn"><i class="fa fa-linkedin" aria-hidden="true"></i></a>  
        </p>
        <p>
            本站总访问量<span id="busuanzi_value_site_pv"></span>次，本站访客数<span id="busuanzi_value_site_uv"></span>人次，本文总阅读量<span id="busuanzi_value_page_pv"></span>次
        </p>
        <p class="power">
            <span>
                Powered by <a href="https://jekyllrb.com/">Jekyll</a> on <a href="https://pages.github.com/">Github Pages</a>.
            </span>
            <span>
                Theme designed by <a href="https://github.com/Gaohaoyang">HyG</a>.
            </span>
        </p>
    </div>
</footer>
<script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>

    <div class="back-to-top">
    <a href="#top" data-scroll>
        <i class="fa fa-arrow-up" aria-hidden="true"></i>
    </a>
</div>

    <script src=" /js/main.js " charset="utf-8"></script>
    <script src=" /js/smooth-scroll.min.js " charset="utf-8"></script>
    <script type="text/javascript">
      smoothScroll.init({
        speed: 500, // Integer. How fast to complete the scroll in milliseconds
        easing: 'easeInOutCubic', // Easing pattern to use
        offset: 20, // Integer. How far to offset the scrolling anchor location in pixels
      });
    </script>
    <!-- <script src=" /js/scroll.min.js " charset="utf-8"></script> -->
  </body>

</html>
